#############################################################################
# gitlab-runner:docker executor options
image: chapeiro/pelago-build:8
# - Add support for docker-in-docker tests / build processes

#############################################################################
variables:
 GIT_SUBMODULE_STRATEGY: recursive
 GIT_DEPTH: "1"

#############################################################################
stages:
 - Configure            # Environment configuration

 - Check Out            # When extra steps are needed to checkout the sources

 - Build Tools          # When extra tools are required

 - Build Libraries      # When extra libraries are required 

 - Build                # Build the project

 - Install              # Install the software so it can be run for tests

 - Test                 # Functional tests

 - Benchmark            # Performance tests (need to be tagged in order to be
                        # run on a noise-free system)

 - Package              # Create archives to be installed

 - Publish              # Publish the packages

 - Build Profile        # Build the project with profiling information

 - Profile              # Profile using known workload

#############################################################################
# Helper Templates

before_script:
 - cache_path=$(pwd | sed "s/^\/builds/\/cache/g")/${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHA}
 - mkdir -p ${cache_path}/build ${cache_path}/opt
 - ln -s ${cache_path}/build build
 - ln -s ${cache_path}/opt opt
 - cp -p -t . ${cache_path}/.*done || true
 # TODO: we should probably remove the next lines and fix that in executor
 - ln -s /data/ssbm100 src/executor/inputs/ssbm100
 - ln -s /data/tpch1 src/executor/inputs/tpch1

after_script:
 # we have to recalculate cache_path here
 - cache_path=$(pwd | sed "s/^\/builds/\/cache/g")/${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHA}
 - cp -p -t ${cache_path} .*done || true

#----------------------------------------------------------------------------
# Caching policies templates

# As we keep the build & install marker files, we need to make sure the
# corresponding binaries are kept as well. While this works for all the C/C++
# projects as we build them outside of the sources, SBT does assume its own
# folder layout, which starts in src/<projetc>. Thus we need to keep a cache
# of the src folder as well.
# FIXME: Can we configure SBT to store the generated files under build?
# FIXME: Can we configure SBT to install the generated JAR under opt?
# FIXME: What about npm?
.cache: &cache
 tags:
  - pelago
 cache:
  key: "${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHA}"
  paths:
   - src/panorama
   - src/planner
   - src/SQLPlanner

.cache: &cache_profile
 cache:
  key: "${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHA}-profile"
  paths:
   - src/panorama
   - src/planner
   - src/SQLPlanner

#----------------------------------------------------------------------------
# Execution restrictions templates
.restrict_precommit: &restrict_precommit
 except:
  - master
  - tags

.restrict_postcommit: &restrict_postcommit
 only:
  - master
  - tags

.restrict_latest: &restrict_latest
 except:
  - tags
 only:
  - master

.restrict_release: &restrict_release
 only:
  - tags

#----------------------------------------------------------------------------
# Job templates
.test: &test
 stage: Test
 cache:
  policy: pull
 <<: *cache
 <<: *restrict_precommit

.benchmark: &benchmark
 stage: Benchmark
 cache:
  policy: pull
 <<: *cache

#############################################################################
# Example jobs
#----------------------------------------------------------------------------

# Workaround until we replace the docker image
.configure-packages:
 stage: Configure
 script:
  - echo "Configure... done"
 <<: *cache
 <<: *restrict_precommit

# Check-out environment
.check-out-submodules:
 stage: Check Out
 script:
  - echo "Check Out... done"
 <<: *cache
 <<: *restrict_precommit

#----------------------------------------------------------------------------
# Setup build environment
build-tools:
 stage: Build Tools
 script:
  - git config --global user.name "Pelago CI"
  - git config --global user.email ""
  - make llvm &> llvm_build_log.txt || (tail -c 512k llvm_build_log.txt && exit 1)
  - echo "Building tools... done"
 <<: *cache
 <<: *restrict_precommit

build-libraries:
 stage: Build Libraries
 script:
  - make external-libs &> external_libs_build_log.txt || (tail -c 512k external_libs_build_log.txt && exit 1)
  - echo "Building external libs... done"
 <<: *cache
 <<: *restrict_precommit

#----------------------------------------------------------------------------
# Setup test environment
build:
 stage: Build
 script:
  - make
  - echo "Building... done"
 <<: *cache
 <<: *restrict_precommit

.install:
 stage: Install
 script:
  - echo "Installing... done"
 <<: *cache
 <<: *restrict_precommit

#----------------------------------------------------------------------------
# Functional Tests
1/10 test - Threads:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-threads --gtest_output="xml:$CWD/report-threads.xml" &> thread_test_log.txt || (tail -c 512k thread_test_log.txt && exit 1)
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml

2/10 test - Plan Parsing:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-plan-parsing --gtest_output="xml:$CWD/report-plan-parsing.xml" &> plan_parsing_test_log.txt || (tail -c 512k plan_parsing_test_log.txt && exit 1)
 allow_failure: true
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml

3/10 test - JSON:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-json --gtest_output="xml:$CWD/report-json.xml" &> json_test_log.txt || (tail -c 512k json_test_log.txt && exit 1)
 allow_failure: true
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml


4/10 test - Joins:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-joins --gtest_output="xml:$CWD/report-joins.xml" &> joins_test_log.txt || (tail -c 512k joins_test_log.txt && exit 1)
 allow_failure: true
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml


5/10 test - Output:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-output --gtest_output="xml:$CWD/report-output.xml" &> output_test_log.txt || (tail -c 512k output_test_log.txt && exit 1)
 allow_failure: true
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml


6/10 test - GPU SSB SF100:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-gpu-ssb --gtest_output="xml:$CWD/report-gpu.xml" &> gpu_ssb_test_log.txt || (tail -c 512k gpu_ssb_test_log.txt && exit 1)
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml


7/10 test - CPU SSB SF100:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-cpu-ssb --gtest_output="xml:$CWD/report-cpu.xml" &> cpu_ssb_test_log.txt || (tail -c 512k cpu_ssb_test_log.txt && exit 1)
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml


8/10 test - Hybrid SSB SF100:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-hyb-ssb --gtest_output="xml:$CWD/report-hyb.xml" &> hyb_ssb_test_log.txt || (tail -c 512k hyb_ssb_test_log.txt && exit 1)
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml

9/10 test - COW:
 <<: *test
 script:
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - export CWD=$(pwd)
  - cd opt/pelago && ./unit-tests-cow --gtest_output="xml:$CWD/report-cow.xml" &> cow_test_log.txt || (tail -c 512k cow_test_log.txt && exit 1)
 artifacts:
  paths:
   - report-*.xml
  reports:
   junit: report-*.xml

10/10 test - Planner:
 <<: *test
 script:
  - rm -r src/planner/inputs
  - ln -s ../../src/executor/inputs src/planner/inputs
  - make &> check_installed.txt || (tail -c 512k check_installed.txt && exit 1) # make sure that everything is compiled
  - cd src/planner && sbt test &> planner_test_log.txt || (tail -c 512k planner_test_log.txt && exit 1)
 artifacts:
  paths:
   - src/planner/target/test-reports/TEST-*.xml
  reports:
   junit: src/planner/target/test-reports/TEST-*.xml

#----------------------------------------------------------------------------
# Performance Tests
# unhide the jobs by removing the leading '.'
bench:
 stage: Benchmark
 script:
  # Check that there is only one credential file
  - '[[ $(ls -1q /data/credentials/*.json | wc -l) -eq 1 ]]'
  # Use parenthesis to keep only the first match
  - ln -s $(ls -1q /data/credentials/*.json | head -n 1) logger_cred.json
  - pip3 install --user --upgrade --requirement src/coordinator/requirements.txt
  - make &> check_installed.txt || (cat check_installed.txt && exit 1) # make sure that everything is compiled
  - python3 src/coordinator/new_coordinator.py --failOnParseError --gspreadsheetId=1FbLJYBfFo7EJnpEYqFokS2WxESX_jWPeapXQKUS_31k --sheetName=`echo ${CI_RUNNER_TAGS} | grep -o 'diascld[0-9]*'` --commitLabel=${CI_COMMIT_SHA} --commitTime="$(TZ=UTC git show -s --format=%cd --date="format-local:%Y-%m-%dT%H:%M:%S%z")" --q0FromSheet < benchmark_sf100.sql &> bench_log.txt || (tail -c 512k bench_log.txt && exit 1)
  - echo "Bench... done"
 <<: *cache
 <<: *restrict_precommit

.1/2 benchmark:
 <<: *benchmark
 script:
  - echo "Benchmark 2... done"

.2/2 benchmark:
 <<: *benchmark
 script:
  - echo "Benchmark 2... done"

#----------------------------------------------------------------------------
# Execution Profiling 
# unhide the jobs by removing the leading '.'
.build profile:
 stage: Build Profile
 script:
  - echo "Building Dependencies... done"
  - echo "Building... done"
  - echo "Installing... done"
 when: manual
 <<: *cache_profile
 <<: *restrict_precommit

.1/2 profile:
 stage: Profile
 script:
  - echo "Running workload 1... done"
  - echo "Extracting profiling statistics"
 when: manual
 cache:
  policy: pull
 <<: *cache_profile
 <<: *restrict_precommit

.2/2 profile:
 stage: Profile
 script:
  - echo "Running workload 2... done"
  - echo "Extracting profiling statistics"
 when: manual
 cache:
  policy: pull
 <<: *cache_profile
 <<: *restrict_precommit

#----------------------------------------------------------------------------
# Packaging & distribution
.tar:
 stage: Package
 script:
  - echo "Generating tar... done"
 <<: *cache
 <<: *restrict_postcommit

.tar:latest:
 stage: Publish
 script:
  - echo "Publishing HEAD... done"
 artifacts:
  name: "$CI_PROJECT_NAME-g$CI_COMMIT_SHA"
  paths:
   - pkg/
 cache:
  policy: pull
 <<: *cache
 <<: *restrict_latest

.tar:release:
 stage: Publish
 script:
  - echo "Publishing release... done"
 artifacts:
  name: "$CI_PROJECT_NAME-$CI_COMMIT_TAG"
  paths:
   - pkg/
 cache:
  policy: pull
 <<: *cache
 <<: *restrict_release
